{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f09e8e9a",
   "metadata": {},
   "source": [
    "# Unsupervised Clustering Methods for Meteorological European Configurations/ Patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8275b925",
   "metadata": {},
   "source": [
    "<span style=\"color: yellow;\"> - prendo solo un sub set dei dati per la velocità (fatto in -> 1.1), poi sarà da prendre tutto il dataset (30.07.25)  </span>  \n",
    "<span style=\"color: yellow;\">- vedere se togliere i percentili in 1.1</span>  \n",
    "<span style=\"color: yellow;\">- In 2.2 vedere se standard scaler va bene o se è emglio usare robust scaler, per ora dovrebbe andare bene</span>  \n",
    "<span style=\"color: yellow;\">- eventualmente nella standardizzazione posso demarcare la cella sopra che ha le funzioni per il calcolo della memoria (no)</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "595b08ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c18b7",
   "metadata": {},
   "source": [
    "## 1 Caricamento Dati e Analisi Iniziale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa42acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ds = xr.open_dataset('era5_2000_2004.grib', engine= 'cfgrib') # XArray DataSet\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "335f3534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of the dataset:\n",
      "   • Variabili: ['z', 't', 'u', 'v']\n",
      "   • Coordinate: ['number', 'time', 'step', 'isobaricInhPa', 'latitude', 'longitude', 'valid_time']\n"
     ]
    }
   ],
   "source": [
    "print(\"Overview of the dataset:\")\n",
    "print(f\"   • Variabili: {list(ds.data_vars.keys())}\")\n",
    "print(f\"   • Coordinate: {list(ds.coords.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97965aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension details:\n",
      "   • Latitude: 201 points (20.0° - 70.0°)\n",
      "   • Longitude: 321 points (-40.0° - 40.0°)\n",
      "   • Time: 1827 steps (2000-01-01 - 2004-12-31)\n",
      "   • Pressure levels: 3 levels ([np.float64(850.0), np.float64(500.0), np.float64(250.0)] hPa)\n",
      "Variables in the dataset:\n",
      "   • z: ('time', 'isobaricInhPa', 'latitude', 'longitude') - Geopotential\n",
      "     └─ Units: m**2 s**-2\n",
      "   • t: ('time', 'isobaricInhPa', 'latitude', 'longitude') - Temperature\n",
      "     └─ Units: K\n",
      "   • u: ('time', 'isobaricInhPa', 'latitude', 'longitude') - U component of wind\n",
      "     └─ Units: m s**-1\n",
      "   • v: ('time', 'isobaricInhPa', 'latitude', 'longitude') - V component of wind\n",
      "     └─ Units: m s**-1\n"
     ]
    }
   ],
   "source": [
    "# Dimenision details\n",
    "print(\"Dimension details:\")\n",
    "if 'latitude' in ds.dims:\n",
    "    print(f\"   • Latitude: {ds.dims['latitude']} points ({ds.latitude.min().values:.1f}° - {ds.latitude.max().values:.1f}°)\")\n",
    "if 'longitude' in ds.dims:\n",
    "    print(f\"   • Longitude: {ds.dims['longitude']} points ({ds.longitude.min().values:.1f}° - {ds.longitude.max().values:.1f}°)\")\n",
    "if 'time' in ds.dims:\n",
    "    print(f\"   • Time: {ds.dims['time']} steps ({pd.to_datetime(ds.time.values[0]).strftime('%Y-%m-%d')} - {pd.to_datetime(ds.time.values[-1]).strftime('%Y-%m-%d')})\")\n",
    "if 'isobaricInhPa' in ds.dims:\n",
    "    print(f\"   • Pressure levels: {ds.dims['isobaricInhPa']} levels ({list(ds.isobaricInhPa.values)} hPa)\")\n",
    "\n",
    "#Variables \n",
    "print(\"Variables in the dataset:\")\n",
    "for var in ds.data_vars:\n",
    "    var_data = ds[var]\n",
    "    print(f\"   • {var}: {var_data.dims} - {var_data.attrs.get('long_name', 'N/A')}\")\n",
    "    print(f\"     └─ Units: {var_data.attrs.get('units', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd087867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSIONALITY:\n",
      "   • Spatial points: 64521\n",
      "   • Total features per timestep: 774,252\n",
      "   • Temporal samples: 1827\n"
     ]
    }
   ],
   "source": [
    "# Total dimensionality\n",
    "total_spatial_points = 1\n",
    "for dim in ['latitude', 'longitude']:\n",
    "    if dim in ds.dims:\n",
    "        total_spatial_points *= ds.dims[dim]\n",
    "\n",
    "total_features = len(ds.data_vars) * ds.dims.get('isobaricInhPa', 1) * total_spatial_points\n",
    "print(\"DIMENSIONALITY:\")\n",
    "print(f\"   • Spatial points: {total_spatial_points}\")\n",
    "print(f\"   • Total features per timestep: {total_features:,}\")\n",
    "print(f\"   • Temporal samples: {ds.dims.get('time', 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d1633",
   "metadata": {},
   "source": [
    "Spatial points: punti griglia nello spazio. \n",
    "La regione osservata è suddivisa in una griglia regolare (0.25° x 0.25°), per ogni punto nella grigli avengono misurate le variabili\n",
    "\n",
    "Total features per timestep: numero di variabili (features) in totale in ogni istante di tempo\n",
    "\n",
    "Temporal samples: punti temporali nel dataset (365 giorni per 5 anni)\n",
    "\n",
    "Posso trasformarlo in una matrice per il clustering:  \n",
    "shape = (temporal_samples, total_features_per_timestep)\n",
    "       = (1827, 774252)  \n",
    "Ogni riga = una mappa meteorologica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5e19da",
   "metadata": {},
   "source": [
    "### 1.1 Check quality in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a4083f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING VALUES: No missing values (previously verified)\n"
     ]
    }
   ],
   "source": [
    "# Missing values check: already verified to be 0\n",
    "print(\"MISSING VALUES: No missing values (previously verified)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f4c385",
   "metadata": {},
   "source": [
    "There are no missing values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e916de27",
   "metadata": {},
   "source": [
    "## 2 Preprocessing and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818f3343",
   "metadata": {},
   "source": [
    "### 2.1 Preparing Data Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd245b9a",
   "metadata": {},
   "source": [
    "#### Struttura inziale dei dati\n",
    "\n",
    "**Scatola** = Dataset   \n",
    "dentro la scatola di sono dei blocchi di fogli\n",
    "\n",
    "**Un blocco di fogli** = un signolo giorno ( da 1 gennaio 2000 a 21 dic 2004)  -> 1827 giorni  \n",
    "il blocco di fogli è formato da 4 fogli uno per ogni variabile\n",
    "\n",
    "**Un foglio contiene i valori di una variabile** =  variabili: u, v, z, t -> 4 variabili  \n",
    "\n",
    "Ogni foglio contiene i valori di quella variabile presi in ogni singolo punto dello 'spazio' definito dalla longitudine e dalla laitudine. Quindi in ogni foglio c'è il valore di quella variabile in ognuno dei 201(lat) × 321(lon). Una specie di tabella.  -> 64521 punti spaziali\n",
    "\n",
    "**Solo che questa tabella di valori è presa per ognuno dei 3 livelli di pressione** = 850 hPa, 500 hPa, 250 hPa -> 3 lv di pressione\n",
    "\n",
    "**TOT= 774252 valori per blocco**   x 1827 giorni\n",
    "\n",
    "Per ogni variabile:  \n",
    "__per ogni livello di pressione:  \n",
    "____per ogni lat:  \n",
    "______per ogni lon:  \n",
    "________prendi il valore  \n",
    "\n",
    "Immagina il foglio come una tabella con 774252 colonne, e solo 1 riga, che rappresenta tutte le misure spaziali per quel giorno.\n",
    "Se metti insieme tutti i 1827 fogli, ottieni una matrice finale di forma (1827, 774252). Ogni riga è un giorno. Ogni colonna è una variabile a una certa posizione e pressione.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c947a7e",
   "metadata": {},
   "source": [
    "#### Struttura finale dei dati\n",
    "\n",
    "L’obiettivo è trasformare tutto in una tabella 2D:\n",
    "\n",
    "           feature_1  feature_2  ...  feature_774252  \n",
    "time_1 →      ...        ...             ...  \n",
    "time_2 →      ...        ...             ...  \n",
    "  ⋮                             \n",
    "time_1827 →   ...        ...             ...  \n",
    "\n",
    "Righe: 1827 (una per ogni timestep)  \n",
    "Colonne: 774,252 (una per ogni combinazione di punto spaziale × variabile)  \n",
    "\n",
    "Organizzazione delle Colonne:  \n",
    "[z_850_lat0_lon0, z_850_lat0_lon1, ..., z_500_lat0_lon0, ..., z_250_lat0_lon0, ...,  \n",
    " t_850_lat0_lon0, t_850_lat0_lon1, ..., t_500_lat0_lon0, ..., t_250_lat0_lon0, ...,  \n",
    " u_850_lat0_lon0, u_850_lat0_lon1, ..., u_500_lat0_lon0, ..., u_250_lat0_lon0, ...,  \n",
    " v_850_lat0_lon0, v_850_lat0_lon1, ..., v_500_lat0_lon0, ..., v_250_lat0_lon0, ...]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62cdc9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to 2D array \n",
    "def prepare_data_matrix(dataset):\n",
    "    \"\"\"Converts xarray dataset to 2D matrix\"\"\"\n",
    "    data_matrices = {}\n",
    "    \n",
    "    for var in dataset.data_vars:\n",
    "        print(f\"   • Processing {var}...\")\n",
    "        var_data = dataset[var]\n",
    "        \n",
    "        # Reorganize dimensions: (time, features)\n",
    "        if 'time' in var_data.dims:\n",
    "            # Stack all non-temporal dimensions\n",
    "            spatial_dims = [dim for dim in var_data.dims if dim != 'time']\n",
    "            if spatial_dims:\n",
    "                stacked = var_data.stack(features=spatial_dims)      # From: var[time=1827, pressure=3, lat=201, lon=321]\n",
    "                matrix = stacked.values  # shape: (time, features)   # To:  var[time=1827, features=193563]  (3×201×321=193563)\n",
    "            else:\n",
    "                matrix = var_data.values.reshape(-1, 1)  # For variables without spatial dimensions\n",
    "        else:\n",
    "            matrix = var_data.values.flatten().reshape(1, -1)\n",
    "        \n",
    "        data_matrices[var] = matrix\n",
    "    \n",
    "    # Concatenate all variables\n",
    "    all_matrices = list(data_matrices.values())\n",
    "    combined_matrix = np.concatenate(all_matrices, axis=1)\n",
    "    \n",
    "    return combined_matrix, data_matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d219d137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPARING DATA MATRIX\n",
      "   • Processing z...\n",
      "   • Processing t...\n",
      "   • Processing t...\n",
      "   • Processing u...\n",
      "   • Processing u...\n",
      "   • Processing v...\n",
      "   • Processing v...\n",
      "Data Matrix: (1827, 774252) (samples, features)\n",
      "Data Matrix: (1827, 774252) (samples, features)\n"
     ]
    }
   ],
   "source": [
    "print(\"PREPARING DATA MATRIX\")\n",
    "X, data_matrices = prepare_data_matrix(ds)\n",
    "print(f\"Data Matrix: {X.shape} (samples, features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39b946f",
   "metadata": {},
   "source": [
    "Prima (per la variabile z):  \n",
    "z[time=0, pressure=850, lat=37.5, lon=12.0] = 1234.5  \n",
    "z[time=0, pressure=500, lat=37.5, lon=12.0] = 5678.9  \n",
    "z[time=0, pressure=250, lat=37.5, lon=12.0] = 9876.1  \n",
    "...  \n",
    "\n",
    "Dopo lo stack:  \n",
    "z[time=0, feature_0] = 1234.5  # (850hPa, lat37.5, lon12.0)  \n",
    "z[time=0, feature_1] = 5678.9  # (500hPa, lat37.5, lon12.0)    \n",
    "z[time=0, feature_2] = 9876.1  # (250hPa, lat37.5, lon12.0)  \n",
    "...  \n",
    "\n",
    "Concatenazione finale:\n",
    "\n",
    "X[time=0] = [z_features (tti i valori di z)... | t_features... | u_features... | v_features...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4036ffae",
   "metadata": {},
   "source": [
    "### 2.2 Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a6144",
   "metadata": {},
   "source": [
    "The next step is to standardize the data for each feature to achieve zero mean and unit variance (standard deviation = 1).Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data   \n",
    "We use Scikit-learn's StandardScaler, which computes the mean and standard deviation across all samples. However, due to the massive dimensionality of our dataset (1827 × 774,252), loading all data into memory simultaneously would cause RAM saturation. To solve thi memory constraint we employ the _.partial_fit()_ method, which enables incremental standardization by processing the data in a streaming fashion - one sample at a time. This approach allows us to handle large datasets that exceed available memory while maintaining the same statistical properties as batch processing.\n",
    "\n",
    "Media e varianza si possono calcolare incrementalmente invece che su tutti i dati in blocco.\n",
    "Equivalente matematico:  \n",
    "Invece di: mean = sum(all_data) / n  \n",
    "Posso usare: mean_new = (mean_old * (n-1) + x_new) / n  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314d72f9",
   "metadata": {},
   "source": [
    "Per ora uso StandardScaler poichè i valori sono ben distribuiti. Ma nel caso la PCA o il Kmeans venissero strai posso provare ad utilizzare RobustScaler che è più robusto agli outliers.\n",
    "  \n",
    "\n",
    "Se ho problemi di RAM, posso:\n",
    "\n",
    "    Ridurre le feature prima della standardizzazione (es. PCA incrementale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ca7022f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STANDARDIZATION - Memory-Safe\n",
      "   • Batch size: 1 \n",
      "   • Phase 1: Calculate statistics (1 sample at a time)...\n",
      "   • partial_fit completed\n",
      "   • Phase 2: In-place transformation...\n",
      "   • partial_fit completed\n",
      "   • Phase 2: In-place transformation...\n",
      "   • Standardization completed!\n",
      "   • Dataset shape: (1827, 774252)\n",
      "   • Standardization verification (per feature):\n",
      "   • Mean (per feature, first 3 feature): [-2.9361894e-08  2.4402997e-08 -5.4808869e-09]\n",
      "   • Std  (per feature, first 3 feature): [0.9999999  1.0000001  0.99999994]\n",
      "   • Mean (per feature, first 10 feature): [-2.9361894e-08  2.4402997e-08 -5.4808869e-09 -2.0227082e-08\n",
      "  6.0942241e-08  1.1744757e-09 -1.3049731e-08  2.7404434e-08\n",
      "  2.1010067e-08 -3.6147753e-08]\n",
      "   • Std  (per feature, first 10 feature): [0.9999999  1.0000001  0.99999994 1.0000004  0.99999994 0.9999995\n",
      " 1.0000007  1.0000004  0.99999964 1.0000002 ]\n",
      "   • Standardization completed!\n",
      "   • Dataset shape: (1827, 774252)\n",
      "   • Standardization verification (per feature):\n",
      "   • Mean (per feature, first 3 feature): [-2.9361894e-08  2.4402997e-08 -5.4808869e-09]\n",
      "   • Std  (per feature, first 3 feature): [0.9999999  1.0000001  0.99999994]\n",
      "   • Mean (per feature, first 10 feature): [-2.9361894e-08  2.4402997e-08 -5.4808869e-09 -2.0227082e-08\n",
      "  6.0942241e-08  1.1744757e-09 -1.3049731e-08  2.7404434e-08\n",
      "  2.1010067e-08 -3.6147753e-08]\n",
      "   • Std  (per feature, first 10 feature): [0.9999999  1.0000001  0.99999994 1.0000004  0.99999994 0.9999995\n",
      " 1.0000007  1.0000004  0.99999964 1.0000002 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"STANDARDIZATION - Memory-Safe\")\n",
    "\n",
    "# Check if data is already standardized to avoid double-processing\n",
    "#print(\"   • Checking if data is already standardized...\")\n",
    "sample_check = X[:3, :10].mean(axis=0)\n",
    "#if np.allclose(sample_check, 0, atol=0.1):\n",
    "#    print(\"  Data appears already standardized - SKIPPING!\")\n",
    "#    print(f\"   • Dataset shape: {X.shape}\")\n",
    "#    print(\"   • If you want to re-standardize, restart kernel and reload data\")\n",
    "#else:\n",
    "#    print(\"  Data in original scale - proceeding with standardization\")\n",
    "\n",
    "batch_size = 1\n",
    "print(f\"   • Batch size: {batch_size} \")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Only proceed if data is not already standardized\n",
    "if not np.allclose(sample_check, 0, atol=0.1):\n",
    "    # Step 1: Calculate statistics one sample at a time\n",
    "    print(\"   • Phase 1: Calculate statistics (1 sample at a time)...\")\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        scaler.partial_fit(X[i:i+1])   # Online computation of mean and std on X for later scaling\n",
    "\n",
    "        # Log every 500 samples to reduce overhead\n",
    "        if i % 500 == 0:\n",
    "            progress = (i / X.shape[0]) * 100\n",
    "            gc.collect()\n",
    "\n",
    "    print(\"   • partial_fit completed\")\n",
    "\n",
    "    # Step 2: Direct in-place transformation \n",
    "    print(\"   • Phase 2: In-place transformation...\")\n",
    "    for i in range(0, X.shape[0]):\n",
    "        # Direct transformation on single row \n",
    "        X[i:i+1] = scaler.transform(X[i:i+1])   # Perform standardization by centering and scaling\n",
    "\n",
    "        # Log every 500 samples to reduce overhead\n",
    "        if i % 500 == 0:\n",
    "            progress = (i / X.shape[0]) * 100\n",
    "\n",
    "            # Garbage collection every 1000 samples\n",
    "            if i % 1000 == 0:\n",
    "                gc.collect()\n",
    "\n",
    "    print(f\"   • Standardization completed!\")\n",
    "\n",
    "print(f\"   • Dataset shape: {X.shape}\")\n",
    "\n",
    "print(\"   • Standardization verification (per feature):\")\n",
    "\n",
    "# Check mean and std for first 3 features\n",
    "mean_per_feature = X[:, :3].mean(axis=0)\n",
    "std_per_feature = X[:, :3].std(axis=0)\n",
    "\n",
    "print(f\"   • Mean (per feature, first 3 feature): {mean_per_feature}\")\n",
    "print(f\"   • Std  (per feature, first 3 feature): {std_per_feature}\")\n",
    "\n",
    "# Check mean and std for first 10 features\n",
    "print(\"   • Mean (per feature, first 10 feature):\", X[:, :10].mean(axis=0))\n",
    "print(\"   • Std  (per feature, first 10 feature):\", X[:, :10].std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b8f68d",
   "metadata": {},
   "source": [
    "PRIMA della standardizzazione:  \n",
    "Colonna 0 (z_850_lat0_lon0): Valori di geopotenziale a 850hPa nel punto (lat0,lon0) per TUTTI i 1827 giorni  \n",
    "Esempio: [10815, 10820, 10805, 10830, ...] → 1827 valori diversi  \n",
    "\n",
    "DOPO la standardizzazione:  \n",
    "Colonna 0: Calcolo mean e std di quei 1827 valori  \n",
    "Esempio: mean=10818, std=15  \n",
    "Ogni valore diventa: (valore_originale - 10818) / 15  \n",
    "Risultato: [-0.2, 0.13, -0.87, 0.8, ...] → media=0, std=1  \n",
    "\n",
    "Cosa Significa:\n",
    "La standardizzazione normalizza ogni punto geografico-variabile rispetto alla sua variabilità temporale.\n",
    "\n",
    "Ad esempio:   \n",
    "Prima: Geopotenziale a Milano a 850hPa varia da 1480 a 1520 metri nei 5 anni  \n",
    "Dopo: Questi valori diventano da -2.1 a +1.8 (media=0, std=1)  \n",
    "Interpretazione: Ora puoi confrontare la \"anomalia\" di Milano con quella di Roma, anche se hanno scale diverse!  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50446c09",
   "metadata": {},
   "source": [
    "Now, an esample of standardization for 3 points is shown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7bdc77",
   "metadata": {},
   "source": [
    "![Esempio Standardizzazione](./standardization_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e14f55",
   "metadata": {},
   "source": [
    "### 2.3 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d42a4",
   "metadata": {},
   "source": [
    "Step 1: PCA Incrementale  \n",
    "Perché farlo:\n",
    "\n",
    "✅ Ti dice quanta varianza catturi con poche componenti  \n",
    "✅ Essenziale per clustering meteorologico (spesso 10-50 componenti bastano)  \n",
    "✅ Memoria gestibile con incremental PCA  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bdf1fa",
   "metadata": {},
   "source": [
    "Key Benefits:  \n",
    "Dimension Reduction: From 774,252 → typically 20-50 components (meteorological standard)  \n",
    "Memory Savings: Reduces memory usage by ~15,000x factor  \n",
    "Clustering-Ready: Prepares data in optimal format for K-means/clustering  \n",
    "Meteorologically Sound: 90% variance retention preserves essential weather patterns  \n",
    "⚡ What to Expect:  \n",
    "The analysis will show that meteorological data typically needs 20-40 components for 90% variance  \n",
    "Massive memory reduction: From ~5.7GB to ~few MB  \n",
    "Preserved Information: Retains all major weather patterns and anomalies  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de965ad3",
   "metadata": {},
   "source": [
    "Analysis Outputs:\n",
    "Variance Thresholds: Shows how many components needed for 80%, 85%, 90%, 95% variance\n",
    "Visualization: Two plots showing individual and cumulative explained variance\n",
    "Component Selection: Automatically selects optimal components for 90% variance retention\n",
    "Memory Efficiency: Compares original vs. PCA memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06390a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INCREMENTAL PCA ANALYSIS\n",
      "   • Target components: 50\n",
      "   • Batch size: 100\n",
      "   • Data shape: (1827, 774252)\n",
      "   • Phase 1: Fitting incremental PCA...\n",
      "      └─ Progress: 5.5%\n",
      "      └─ Progress: 5.5%\n",
      "      └─ Progress: 60.2%\n",
      "      └─ Progress: 60.2%\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"INCREMENTAL PCA ANALYSIS\")\n",
    "\n",
    "# Memory-efficient PCA with incremental processing\n",
    "# Start with analysis of many components to understand variance distribution\n",
    "n_components = 50   # Reduced to 50 components for safety with batch processing\n",
    "batch_size = 100    # Increased batch size to accommodate more components\n",
    "\n",
    "print(f\"   • Target components: {n_components}\")\n",
    "print(f\"   • Batch size: {batch_size}\")\n",
    "print(f\"   • Data shape: {X.shape}\")\n",
    "\n",
    "# Ensure batch_size doesn't exceed total samples\n",
    "if batch_size > X.shape[0]:\n",
    "    batch_size = X.shape[0]\n",
    "    print(f\"   • Adjusted batch size to total samples: {batch_size}\")\n",
    "\n",
    "# Initialize IncrementalPCA\n",
    "ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n",
    "\n",
    "print(\"   • Phase 1: Fitting incremental PCA...\")\n",
    "# Fit the PCA incrementally\n",
    "for i in range(0, X.shape[0], batch_size):\n",
    "    batch = X[i:i+batch_size]\n",
    "    ipca.partial_fit(batch)\n",
    "    \n",
    "    # Progress tracking\n",
    "    if i % (batch_size * 10) == 0:  # Every 10 batches\n",
    "        progress = min(100, (i + batch_size) / X.shape[0] * 100)\n",
    "        print(f\"      └─ Progress: {progress:.1f}%\")\n",
    "        gc.collect()\n",
    "\n",
    "print(\"   • Phase 2: Transforming data...\")\n",
    "# Transform data in batches to avoid memory issues\n",
    "X_pca = np.zeros((X.shape[0], n_components))\n",
    "\n",
    "for i in range(0, X.shape[0], batch_size):\n",
    "    end_idx = min(i + batch_size, X.shape[0])\n",
    "    batch = X[i:end_idx]\n",
    "    X_pca[i:end_idx] = ipca.transform(batch)\n",
    "    \n",
    "    # Progress tracking\n",
    "    if i % (batch_size * 10) == 0:  # Every 10 batches\n",
    "        progress = min(100, (i + batch_size) / X.shape[0] * 100)\n",
    "        print(f\"      └─ Progress: {progress:.1f}%\")\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"   • PCA transformation completed!\")\n",
    "print(f\"   • Original shape: {X.shape}\")\n",
    "print(f\"   • PCA shape: {X_pca.shape}\")\n",
    "\n",
    "# Analyze explained variance\n",
    "explained_variance_ratio = ipca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "print(\"\\n   • VARIANCE ANALYSIS:\")\n",
    "print(f\"   • Total explained variance ({n_components} components): {cumulative_variance[-1]:.3f}\")\n",
    "\n",
    "# Find components needed for different variance thresholds\n",
    "thresholds = [0.80, 0.85, 0.90, 0.95]\n",
    "for threshold in thresholds:\n",
    "    n_comp_needed = np.argmax(cumulative_variance >= threshold) + 1\n",
    "    if cumulative_variance[-1] >= threshold:\n",
    "        print(f\"   • {threshold*100}% variance: {n_comp_needed} components\")\n",
    "    else:\n",
    "        print(f\"   • {threshold*100}% variance: >{n_components} components needed\")\n",
    "\n",
    "# Show variance distribution for first components\n",
    "print(f\"\\n   • First 10 components variance: {explained_variance_ratio[:10]}\")\n",
    "print(f\"   • First 20 components cumulative: {cumulative_variance[19]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c7e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of explained variance\n",
    "print(\"VARIANCE VISUALIZATION\")\n",
    "\n",
    "# Create plots for variance analysis\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Individual explained variance ratio\n",
    "ax1.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, 'b-', alpha=0.7)\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Explained Variance Ratio')\n",
    "ax1.set_title('Individual Explained Variance per Component')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(1, 50)  # Focus on first 50 components\n",
    "\n",
    "# Plot 2: Cumulative explained variance\n",
    "ax2.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'r-', linewidth=2)\n",
    "ax2.axhline(y=0.80, color='orange', linestyle='--', alpha=0.7, label='80%')\n",
    "ax2.axhline(y=0.90, color='green', linestyle='--', alpha=0.7, label='90%')\n",
    "ax2.axhline(y=0.95, color='purple', linestyle='--', alpha=0.7, label='95%')\n",
    "ax2.set_xlabel('Number of Components')\n",
    "ax2.set_ylabel('Cumulative Explained Variance')\n",
    "ax2.set_title('Cumulative Explained Variance')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(1, n_components)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed analysis for optimal component selection\n",
    "print(\"\\n   • COMPONENT SELECTION RECOMMENDATIONS:\")\n",
    "print(\"   • For meteorological clustering, typical ranges:\")\n",
    "print(\"   • Conservative (high quality): 80-90% variance\")\n",
    "print(\"   • Moderate (balanced): 90-95% variance\") \n",
    "print(\"   • Aggressive (dimension reduction): 95%+ variance\")\n",
    "\n",
    "# Memory usage comparison\n",
    "original_memory = X.nbytes / (1024**3)  # GB\n",
    "pca_memory = X_pca.nbytes / (1024**3)   # GB\n",
    "reduction_factor = X.shape[1] / X_pca.shape[1]\n",
    "\n",
    "print(f\"\\n   • MEMORY EFFICIENCY:\")\n",
    "print(f\"   • Original data: {original_memory:.2f} GB\")\n",
    "print(f\"   • PCA data: {pca_memory:.2f} GB\") \n",
    "print(f\"   • Memory reduction: {reduction_factor:.1f}x\")\n",
    "print(f\"   • Storage efficiency: {(1 - pca_memory/original_memory)*100:.1f}% saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d386683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component selection for final analysis\n",
    "print(\"COMPONENT SELECTION FOR CLUSTERING\")\n",
    "\n",
    "# Determine optimal number of components based on variance analysis\n",
    "target_variance = 0.90  # 90% variance retention (good balance for meteorology)\n",
    "optimal_components = np.argmax(cumulative_variance >= target_variance) + 1\n",
    "\n",
    "print(f\"   • Target variance retention: {target_variance*100}%\")\n",
    "print(f\"   • Optimal components: {optimal_components}\")\n",
    "print(f\"   • Actual variance captured: {cumulative_variance[optimal_components-1]:.3f}\")\n",
    "\n",
    "# Create reduced dataset with optimal components\n",
    "X_reduced = X_pca[:, :optimal_components]\n",
    "print(f\"   • Reduced data shape: {X_reduced.shape}\")\n",
    "\n",
    "# Save memory by cleaning up intermediate results\n",
    "del X_pca  # Keep only the reduced version\n",
    "gc.collect()\n",
    "\n",
    "print(f\"   • Memory cleanup completed\")\n",
    "print(f\"   • Ready for clustering analysis!\")\n",
    "\n",
    "# Summary statistics of the reduced dataset\n",
    "print(f\"\\n   • REDUCED DATASET SUMMARY:\")\n",
    "print(f\"   • Shape: {X_reduced.shape}\")\n",
    "print(f\"   • Memory: {X_reduced.nbytes / (1024**2):.1f} MB\")\n",
    "print(f\"   • Variance captured: {cumulative_variance[optimal_components-1]:.3f}\")\n",
    "print(f\"   • Dimension reduction: {X.shape[1]} → {optimal_components} features\")\n",
    "print(f\"   • Compression ratio: {X.shape[1]/optimal_components:.1f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c91f1",
   "metadata": {},
   "source": [
    "Step 2: Analisi Varianza  (Plot varianza spiegata)\n",
    "Guarda quante componenti servono per 90% varianza  \n",
    "Plot: varianza cumulativa vs numero componenti  \n",
    "Scegli dimensioni finali (probabilmente 20-80 componenti)  \n",
    "\n",
    "3. Correlazione tra VARIABILI (non tutte le features)  \n",
    "Analizza correlazioni tra le 4 variabili meteorologiche aggregate  \n",
    "Invece di 774k×774k → 4×4 matrice gestibile  \n",
    "Correlazioni tra variabili meteorologiche (aggregate per livello)\n",
    "NON matrice completa (troppo grande)\n",
    "Perché limitarsi:\n",
    "\n",
    "❌ Matrice 774k×774k = 2.4 TB di memoria → IMPOSSIBILE  \n",
    "✅ Correlazione z-u-v-t è meteorologicamente significat  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grib_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
