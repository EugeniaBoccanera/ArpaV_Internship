{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f09e8e9a",
   "metadata": {},
   "source": [
    "# Unsupervised Clustering Methods for Meteorological European Configurations/ Patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8275b925",
   "metadata": {},
   "source": [
    "<span style=\"color: yellow;\"> - prendo solo un sub set dei dati per la velocità (fatto in -> 1.1), poi sarà da prendre tutto il dataset (30.07.25)  </span>  \n",
    "<span style=\"color: yellow;\">- vedere se togliere i percentili in 1.1</span>  \n",
    "<span style=\"color: yellow;\">- In 2.2 vedere se standard scaler va bene o se è emglio usare robust scaler, per ora dovrebbe andare bene</span>  \n",
    "<span style=\"color: yellow;\">- eventualmente nella standardizzazione posso demarcare la cella sopra che ha le funzioni per il calcolo della memoria (no)</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "595b08ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c18b7",
   "metadata": {},
   "source": [
    "## 1 Caricamento Dati e Analisi Iniziale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa42acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ds = xr.open_dataset('era5_2000_2004.grib', engine= 'cfgrib') # XArray DataSet\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "335f3534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of the dataset:\n",
      "   • Variabili: ['z', 't', 'u', 'v']\n",
      "   • Coordinate: ['number', 'time', 'step', 'isobaricInhPa', 'latitude', 'longitude', 'valid_time']\n"
     ]
    }
   ],
   "source": [
    "print(\"Overview of the dataset:\")\n",
    "print(f\"   • Variabili: {list(ds.data_vars.keys())}\")\n",
    "print(f\"   • Coordinate: {list(ds.coords.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97965aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension details:\n",
      "   • Latitude: 201 points (20.0° - 70.0°)\n",
      "   • Longitude: 321 points (-40.0° - 40.0°)\n",
      "   • Time: 1827 steps (2000-01-01 - 2004-12-31)\n",
      "   • Pressure levels: 3 levels ([np.float64(850.0), np.float64(500.0), np.float64(250.0)] hPa)\n",
      "Variables in the dataset:\n",
      "   • z: ('time', 'isobaricInhPa', 'latitude', 'longitude') - Geopotential\n",
      "     └─ Units: m**2 s**-2\n",
      "   • t: ('time', 'isobaricInhPa', 'latitude', 'longitude') - Temperature\n",
      "     └─ Units: K\n",
      "   • u: ('time', 'isobaricInhPa', 'latitude', 'longitude') - U component of wind\n",
      "     └─ Units: m s**-1\n",
      "   • v: ('time', 'isobaricInhPa', 'latitude', 'longitude') - V component of wind\n",
      "     └─ Units: m s**-1\n"
     ]
    }
   ],
   "source": [
    "# Dimenision details\n",
    "print(\"Dimension details:\")\n",
    "if 'latitude' in ds.dims:\n",
    "    print(f\"   • Latitude: {ds.dims['latitude']} points ({ds.latitude.min().values:.1f}° - {ds.latitude.max().values:.1f}°)\")\n",
    "if 'longitude' in ds.dims:\n",
    "    print(f\"   • Longitude: {ds.dims['longitude']} points ({ds.longitude.min().values:.1f}° - {ds.longitude.max().values:.1f}°)\")\n",
    "if 'time' in ds.dims:\n",
    "    print(f\"   • Time: {ds.dims['time']} steps ({pd.to_datetime(ds.time.values[0]).strftime('%Y-%m-%d')} - {pd.to_datetime(ds.time.values[-1]).strftime('%Y-%m-%d')})\")\n",
    "if 'isobaricInhPa' in ds.dims:\n",
    "    print(f\"   • Pressure levels: {ds.dims['isobaricInhPa']} levels ({list(ds.isobaricInhPa.values)} hPa)\")\n",
    "\n",
    "#Variables \n",
    "print(\"Variables in the dataset:\")\n",
    "for var in ds.data_vars:\n",
    "    var_data = ds[var]\n",
    "    print(f\"   • {var}: {var_data.dims} - {var_data.attrs.get('long_name', 'N/A')}\")\n",
    "    print(f\"     └─ Units: {var_data.attrs.get('units', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd087867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSIONALITY:\n",
      "   • Spatial points: 64521\n",
      "   • Total features per timestep: 774,252\n",
      "   • Temporal samples: 1827\n"
     ]
    }
   ],
   "source": [
    "# Total dimensionality\n",
    "total_spatial_points = 1\n",
    "for dim in ['latitude', 'longitude']:\n",
    "    if dim in ds.dims:\n",
    "        total_spatial_points *= ds.dims[dim]\n",
    "\n",
    "total_features = len(ds.data_vars) * ds.dims.get('isobaricInhPa', 1) * total_spatial_points\n",
    "print(\"DIMENSIONALITY:\")\n",
    "print(f\"   • Spatial points: {total_spatial_points}\")\n",
    "print(f\"   • Total features per timestep: {total_features:,}\")\n",
    "print(f\"   • Temporal samples: {ds.dims.get('time', 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d1633",
   "metadata": {},
   "source": [
    "Spatial points: punti griglia nello spazio. \n",
    "La regione osservata è suddivisa in una griglia regolare (0.25° x 0.25°), per ogni punto nella grigli avengono misurate le variabili\n",
    "\n",
    "Total features per timestep: numero di variabili (features) in totale in ogni istante di tempo\n",
    "\n",
    "Temporal samples: punti temporali nel dataset (365 giorni per 5 anni)\n",
    "\n",
    "Posso trasformarlo in una matrice per il clustering:  \n",
    "shape = (temporal_samples, total_features_per_timestep)\n",
    "       = (1827, 774252)  \n",
    "Ogni riga = una mappa meteorologica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5e19da",
   "metadata": {},
   "source": [
    "### 1.1 Check quality in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a4083f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING VALUES: No missing values (previously verified)\n"
     ]
    }
   ],
   "source": [
    "# Missing values check: already verified to be 0\n",
    "print(\"MISSING VALUES: No missing values (previously verified)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f4c385",
   "metadata": {},
   "source": [
    "There are no missing values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e916de27",
   "metadata": {},
   "source": [
    "## 2 Preprocessing and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818f3343",
   "metadata": {},
   "source": [
    "### 2.1 Preparing Data Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd245b9a",
   "metadata": {},
   "source": [
    "#### Struttura inziale dei dati\n",
    "\n",
    "**Scatola** = Dataset   \n",
    "dentro la scatola di sono dei blocchi di fogli\n",
    "\n",
    "**Un blocco di fogli** = un signolo giorno ( da 1 gennaio 2000 a 21 dic 2004)  -> 1827 giorni  \n",
    "il blocco di fogli è formato da 4 fogli uno per ogni variabile\n",
    "\n",
    "**Un foglio contiene i valori di una variabile** =  variabili: u, v, z, t -> 4 variabili  \n",
    "\n",
    "Ogni foglio contiene i valori di quella variabile presi in ogni singolo punto dello 'spazio' definito dalla longitudine e dalla laitudine. Quindi in ogni foglio c'è il valore di quella variabile in ognuno dei 201(lat) × 321(lon). Una specie di tabella.  -> 64521 punti spaziali\n",
    "\n",
    "**Solo che questa tabella di valori è presa per ognuno dei 3 livelli di pressione** = 850 hPa, 500 hPa, 250 hPa -> 3 lv di pressione\n",
    "\n",
    "**TOT= 774252 valori per blocco**   x 1827 giorni\n",
    "\n",
    "Per ogni variabile:  \n",
    "__per ogni livello di pressione:  \n",
    "____per ogni lat:  \n",
    "______per ogni lon:  \n",
    "________prendi il valore  \n",
    "\n",
    "Immagina il foglio come una tabella con 774252 colonne, e solo 1 riga, che rappresenta tutte le misure spaziali per quel giorno.\n",
    "Se metti insieme tutti i 1827 fogli, ottieni una matrice finale di forma (1827, 774252). Ogni riga è un giorno. Ogni colonna è una variabile a una certa posizione e pressione.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c947a7e",
   "metadata": {},
   "source": [
    "#### Struttura finale dei dati\n",
    "\n",
    "L’obiettivo è trasformare tutto in una tabella 2D:\n",
    "\n",
    "           feature_1  feature_2  ...  feature_774252  \n",
    "time_1 →      ...        ...             ...  \n",
    "time_2 →      ...        ...             ...  \n",
    "  ⋮                             \n",
    "time_1827 →   ...        ...             ...  \n",
    "\n",
    "Righe: 1827 (una per ogni timestep)  \n",
    "Colonne: 774,252 (una per ogni combinazione di punto spaziale × variabile)  \n",
    "\n",
    "Ogni colonna è, ad esempio:  \n",
    "\"z_850hPa_lat37.5_lon12.0\"  \n",
    "\"u_500hPa_lat42.5_lon18.0\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62cdc9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to 2D array \n",
    "def prepare_data_matrix(dataset):\n",
    "    \"\"\"Converts xarray dataset to 2D matrix\"\"\"\n",
    "    data_matrices = {}\n",
    "    \n",
    "    for var in dataset.data_vars:\n",
    "        print(f\"   • Processing {var}...\")\n",
    "        var_data = dataset[var]\n",
    "        \n",
    "        # Reorganize dimensions: (time, features)\n",
    "        if 'time' in var_data.dims:\n",
    "            # Stack all non-temporal dimensions\n",
    "            spatial_dims = [dim for dim in var_data.dims if dim != 'time']\n",
    "            if spatial_dims:\n",
    "                stacked = var_data.stack(features=spatial_dims)      # From: var[time=1827, pressure=3, lat=201, lon=321]\n",
    "                matrix = stacked.values  # shape: (time, features)   # To:  var[time=1827, features=193563]  (3×201×321=193563)\n",
    "            else:\n",
    "                matrix = var_data.values.reshape(-1, 1)  # For variables without spatial dimensions\n",
    "        else:\n",
    "            matrix = var_data.values.flatten().reshape(1, -1)\n",
    "        \n",
    "        data_matrices[var] = matrix\n",
    "    \n",
    "    # Concatenate all variables\n",
    "    all_matrices = list(data_matrices.values())\n",
    "    combined_matrix = np.concatenate(all_matrices, axis=1)\n",
    "    \n",
    "    return combined_matrix, data_matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d219d137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPARING DATA MATRIX\n",
      "   • Processing z...\n",
      "   • Processing t...\n",
      "   • Processing t...\n",
      "   • Processing u...\n",
      "   • Processing u...\n",
      "   • Processing v...\n",
      "   • Processing v...\n",
      "Data Matrix: (1827, 774252) (samples, features)\n",
      "Data Matrix: (1827, 774252) (samples, features)\n"
     ]
    }
   ],
   "source": [
    "print(\"PREPARING DATA MATRIX\")\n",
    "X, data_matrices = prepare_data_matrix(ds)\n",
    "print(f\"Data Matrix: {X.shape} (samples, features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39b946f",
   "metadata": {},
   "source": [
    "Prima (per la variabile z):  \n",
    "z[time=0, pressure=850, lat=37.5, lon=12.0] = 1234.5  \n",
    "z[time=0, pressure=500, lat=37.5, lon=12.0] = 5678.9  \n",
    "z[time=0, pressure=250, lat=37.5, lon=12.0] = 9876.1  \n",
    "...  \n",
    "\n",
    "Dopo lo stack:  \n",
    "z[time=0, feature_0] = 1234.5  # (850hPa, lat37.5, lon12.0)  \n",
    "z[time=0, feature_1] = 5678.9  # (500hPa, lat37.5, lon12.0)    \n",
    "z[time=0, feature_2] = 9876.1  # (250hPa, lat37.5, lon12.0)  \n",
    "...  \n",
    "\n",
    "Concatenazione finale:\n",
    "\n",
    "X[time=0] = [z_features (tti i valori di z)... | t_features... | u_features... | v_features...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4036ffae",
   "metadata": {},
   "source": [
    "### 2.2 Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a6144",
   "metadata": {},
   "source": [
    "The next step is to standardize the data for each feature to achieve zero mean and unit variance (standard deviation = 1).Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data   \n",
    "We use Scikit-learn's StandardScaler, which computes the mean and standard deviation across all samples. However, due to the massive dimensionality of our dataset (1827 × 774,252), loading all data into memory simultaneously would cause RAM saturation. To solve thi memory constraint we employ the _.partial_fit()_ method, which enables incremental standardization by processing the data in a streaming fashion - one sample at a time. This approach allows us to handle large datasets that exceed available memory while maintaining the same statistical properties as batch processing.\n",
    "\n",
    "Media e varianza si possono calcolare incrementalmente invece che su tutti i dati in blocco.\n",
    "Equivalente matematico:  \n",
    "Invece di: mean = sum(all_data) / n  \n",
    "Posso usare: mean_new = (mean_old * (n-1) + x_new) / n  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314d72f9",
   "metadata": {},
   "source": [
    "Per ora uso StandardScaler poichè i valori sono ben distribuiti. Ma nel caso la PCA o il Kmeans venissero strai posso provare ad utilizzare RobustScaler che è più robusto agli outliers.\n",
    "  \n",
    "\n",
    "Se ho problemi di RAM, posso:\n",
    "\n",
    "    Ridurre le feature prima della standardizzazione (es. PCA incrementale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ca7022f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STANDARDIZATION - Memory-Safe\n",
      "   • Batch size: 1 \n",
      "   • Phase 1: Calculate statistics (1 sample at a time)...\n",
      "   • partial_fit completed\n",
      "   • Phase 2: In-place transformation...\n",
      "   • partial_fit completed\n",
      "   • Phase 2: In-place transformation...\n",
      "   • Standardization completed!\n",
      "   • Dataset shape: (1827, 774252)\n",
      "   • Standardization verification (per feature):\n",
      "   • Mean (per feature, first 3 feature): [-2.9361894e-08  2.4402997e-08 -5.4808869e-09]\n",
      "   • Std  (per feature, first 3 feature): [0.9999999  1.0000001  0.99999994]\n",
      "   • Mean (per feature, first 10 feature): [-2.9361894e-08  2.4402997e-08 -5.4808869e-09 -2.0227082e-08\n",
      "  6.0942241e-08  1.1744757e-09 -1.3049731e-08  2.7404434e-08\n",
      "  2.1010067e-08 -3.6147753e-08]\n",
      "   • Std  (per feature, first 10 feature): [0.9999999  1.0000001  0.99999994 1.0000004  0.99999994 0.9999995\n",
      " 1.0000007  1.0000004  0.99999964 1.0000002 ]\n",
      "   • Standardization completed!\n",
      "   • Dataset shape: (1827, 774252)\n",
      "   • Standardization verification (per feature):\n",
      "   • Mean (per feature, first 3 feature): [-2.9361894e-08  2.4402997e-08 -5.4808869e-09]\n",
      "   • Std  (per feature, first 3 feature): [0.9999999  1.0000001  0.99999994]\n",
      "   • Mean (per feature, first 10 feature): [-2.9361894e-08  2.4402997e-08 -5.4808869e-09 -2.0227082e-08\n",
      "  6.0942241e-08  1.1744757e-09 -1.3049731e-08  2.7404434e-08\n",
      "  2.1010067e-08 -3.6147753e-08]\n",
      "   • Std  (per feature, first 10 feature): [0.9999999  1.0000001  0.99999994 1.0000004  0.99999994 0.9999995\n",
      " 1.0000007  1.0000004  0.99999964 1.0000002 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"STANDARDIZATION - Memory-Safe\")\n",
    "\n",
    "# Check if data is already standardized to avoid double-processing\n",
    "#print(\"   • Checking if data is already standardized...\")\n",
    "sample_check = X[:3, :10].mean(axis=0)\n",
    "#if np.allclose(sample_check, 0, atol=0.1):\n",
    "#    print(\"  Data appears already standardized - SKIPPING!\")\n",
    "#    print(f\"   • Dataset shape: {X.shape}\")\n",
    "#    print(\"   • If you want to re-standardize, restart kernel and reload data\")\n",
    "#else:\n",
    "#    print(\"  Data in original scale - proceeding with standardization\")\n",
    "\n",
    "batch_size = 1\n",
    "print(f\"   • Batch size: {batch_size} \")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Only proceed if data is not already standardized\n",
    "if not np.allclose(sample_check, 0, atol=0.1):\n",
    "    # Step 1: Calculate statistics one sample at a time\n",
    "    print(\"   • Phase 1: Calculate statistics (1 sample at a time)...\")\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        scaler.partial_fit(X[i:i+1])   # Online computation of mean and std on X for later scaling\n",
    "\n",
    "        # Log every 500 samples to reduce overhead\n",
    "        if i % 500 == 0:\n",
    "            progress = (i / X.shape[0]) * 100\n",
    "            gc.collect()\n",
    "\n",
    "    print(\"   • partial_fit completed\")\n",
    "\n",
    "    # Step 2: Direct in-place transformation \n",
    "    print(\"   • Phase 2: In-place transformation...\")\n",
    "    for i in range(0, X.shape[0]):\n",
    "        # Direct transformation on single row \n",
    "        X[i:i+1] = scaler.transform(X[i:i+1])   # Perform standardization by centering and scaling\n",
    "\n",
    "        # Log every 500 samples to reduce overhead\n",
    "        if i % 500 == 0:\n",
    "            progress = (i / X.shape[0]) * 100\n",
    "\n",
    "            # Garbage collection every 1000 samples\n",
    "            if i % 1000 == 0:\n",
    "                gc.collect()\n",
    "\n",
    "    print(f\"   • Standardization completed!\")\n",
    "\n",
    "print(f\"   • Dataset shape: {X.shape}\")\n",
    "\n",
    "print(\"   • Standardization verification (per feature):\")\n",
    "\n",
    "# Check mean and std for first 3 features\n",
    "mean_per_feature = X[:, :3].mean(axis=0)\n",
    "std_per_feature = X[:, :3].std(axis=0)\n",
    "\n",
    "print(f\"   • Mean (per feature, first 3 feature): {mean_per_feature}\")\n",
    "print(f\"   • Std  (per feature, first 3 feature): {std_per_feature}\")\n",
    "\n",
    "# Check mean and std for first 10 features\n",
    "print(\"   • Mean (per feature, first 10 feature):\", X[:, :10].mean(axis=0))\n",
    "print(\"   • Std  (per feature, first 10 feature):\", X[:, :10].std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50446c09",
   "metadata": {},
   "source": [
    "Now, an esample of standardization for 3 points is shown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7bdc77",
   "metadata": {},
   "source": [
    "![Esempio Standardizzazione](./standardization_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e14f55",
   "metadata": {},
   "source": [
    "### 2.3 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d42a4",
   "metadata": {},
   "source": [
    "Perché farlo:\n",
    "\n",
    "✅ Ti dice quanta varianza catturi con poche componenti  \n",
    "✅ Essenziale per clustering meteorologico (spesso 10-50 componenti bastano)  \n",
    "✅ Memoria gestibile con incremental PCA  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c91f1",
   "metadata": {},
   "source": [
    "Step 2: Analisi Varianza  (Plot varianza spiegata)\n",
    "Guarda quante componenti servono per 90% varianza  \n",
    "Plot: varianza cumulativa vs numero componenti  \n",
    "Scegli dimensioni finali (probabilmente 20-80 componenti)  \n",
    "\n",
    "3. Correlazione tra VARIABILI (non tutte le features)  \n",
    "Analizza correlazioni tra le 4 variabili meteorologiche aggregate  \n",
    "Invece di 774k×774k → 4×4 matrice gestibile  \n",
    "Correlazioni tra variabili meteorologiche (aggregate per livello)\n",
    "NON matrice completa (troppo grande)\n",
    "Perché limitarsi:\n",
    "\n",
    "❌ Matrice 774k×774k = 2.4 TB di memoria → IMPOSSIBILE  \n",
    "✅ Correlazione z-u-v-t è meteorologicamente significat  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c75357c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grib_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
