{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc42e311",
   "metadata": {},
   "source": [
    "### Notebook to produce the dataset with 60years already normalized and reduced(pca) with the results obtained from the analysis over the 60y_jump_6_days\n",
    "\n",
    "OUTPUT : matrix with 60y (1961-2020) normalized and with pca applied\n",
    "- X_pca_60y.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1536d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import gc\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import importlib\n",
    "import utils.data_processing\n",
    "import utils.visualization\n",
    "importlib.reload(utils.data_processing)\n",
    "importlib.reload(utils.visualization)\n",
    "\n",
    "# Import the functions \n",
    "from utils.data_processing import prepare_data_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4c9e086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t']\n",
      "['z']\n",
      "Overview of the combined dataset:\n",
      "   • Variables: ['t', 'z']\n",
      "   • Coordinates: ['number', 'time', 'step', 'latitude', 'longitude', 'valid_time']\n",
      "   • Shape: FrozenMappingWarningOnValuesAccess({'time': 3653, 'latitude': 201, 'longitude': 321})\n"
     ]
    }
   ],
   "source": [
    "# Return to the original dataset to process the 10 years data \n",
    "\n",
    "file_name= \"10y/era5_2011_2020_t850_z500.grib\"    ###### CHANGE WHEN CHANGING FILE\n",
    "year = \"2011_2020\"                            ###### CHANGE WHEN CHANGING FILE\n",
    "\n",
    "# Load only the t variable\n",
    "ds_t = xr.open_dataset(file_name, engine='cfgrib', filter_by_keys={'shortName': 't'})\n",
    "t_850 = ds_t['t']\n",
    "\n",
    "print(list(ds_t.data_vars.keys()))\n",
    "\n",
    "# Load only the z variable\n",
    "ds_z = xr.open_dataset(file_name, engine='cfgrib', filter_by_keys={'shortName': 'z'})\n",
    "z_500 = ds_z['z']\n",
    "print(list(ds_z.data_vars.keys()))\n",
    "\n",
    "t_850 = t_850.reset_coords('isobaricInhPa', drop=True)\n",
    "z_500 = z_500.reset_coords('isobaricInhPa', drop=True)\n",
    "ds_filtered = xr.Dataset({\n",
    "    't': t_850,\n",
    "    'z': z_500\n",
    "})\n",
    "print(\"Overview of the combined dataset:\")\n",
    "print(f\"   • Variables: {list(ds_filtered.data_vars.keys())}\")\n",
    "print(f\"   • Coordinates: {list(ds_filtered.coords.keys())}\")\n",
    "print(f\"   • Shape: {ds_filtered.dims}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e03924af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing t...\n",
      "     → t: ('time', 'latitude', 'longitude') → (3653, 64521)\n",
      "Processing z...\n",
      "     → z: ('time', 'latitude', 'longitude') → (3653, 64521)\n",
      "\n",
      "Combined matrix shape: (3653, 129042)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, _ = prepare_data_matrix(ds_filtered)\n",
    "\n",
    "\n",
    "t_mean = np.load(\"Mid_result_to_save/t_mean_60y.npy\")\n",
    "t_std = np.load(\"Mid_result_to_save/t_std_60y.npy\") \n",
    "z_mean = np.load(\"Mid_result_to_save/z_mean_60y.npy\")\n",
    "z_std = np.load(\"Mid_result_to_save/z_std_60y.npy\")\n",
    "\n",
    "# Compute spatial dimensions\n",
    "n_lat = 201  \n",
    "n_lon = 321  \n",
    "spatial_size = n_lat * n_lon\n",
    "\n",
    "# Separate temperature and geopotential data\n",
    "X_temperature = X[:, :spatial_size]\n",
    "X_geopotential = X[:, spatial_size:]\n",
    "\n",
    "X_temperature_std = (X_temperature - t_mean) / t_std\n",
    "X_geopotential_std = (X_geopotential - z_mean) / z_std\n",
    "\n",
    "X_std = np.concatenate([X_temperature_std, X_geopotential_std], axis=1)\n",
    "\n",
    "del ds_filtered, X, X_temperature, X_geopotential, X_temperature_std, X_geopotential_std\n",
    "gc.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e650b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dati ridotti salvati in Mid_result_to_save/X_pca_2011_2020.npy\n"
     ]
    }
   ],
   "source": [
    "# Load the PCA model\n",
    "ipca = joblib.load(\"Mid_result_to_save/ipca_60y.pkl\")\n",
    "X_reduced = ipca.transform(X_std)\n",
    "\n",
    "# Save the reduced data\n",
    "np.save(f\"Mid_result_to_save/X_pca_{year}.npy\", X_reduced)\n",
    "print(f\"Dati ridotti salvati in Mid_result_to_save/X_pca_{year}.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb44189",
   "metadata": {},
   "source": [
    "_The previous code as been applied to the two files: era5_1994_2008.grib and era5_2009_2023.grib. Now I have obtained the two matrices X_pca_...period..., and now I merge them in order to obtain the final 30 years matrix with the normalization and pca applied_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75cc6f8",
   "metadata": {},
   "source": [
    "______  ____________  ________  _________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1222b620",
   "metadata": {},
   "source": [
    "#### 60 years matrix\n",
    "Now let's merge together all the X_pca_files for the ten years in order to form the dataset over the 60 years already normalized and with PCs.  \n",
    "\n",
    "<span style=\"color: red;\">Run the following cell only after havin produce (with the above cells) all the X_pca_... needed</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "146230f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape unified matrix: (21915, 20)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X1 = np.load(\"Mid_result_to_save/X_pca_1961_1970.npy\")\n",
    "X2 = np.load(\"Mid_result_to_save/X_pca_1971_1980.npy\")\n",
    "X3= np.load(\"Mid_result_to_save/X_pca_1981_1990.npy\")\n",
    "X4= np.load(\"Mid_result_to_save/X_pca_1991_2000.npy\")\n",
    "X5= np.load(\"Mid_result_to_save/X_pca_2001_2010.npy\")\n",
    "X6= np.load(\"Mid_result_to_save/X_pca_2011_2020.npy\")\n",
    "\n",
    "X_pca_60y = np.concatenate([X1, X2, X3, X4, X5, X6], axis=0)\n",
    "\n",
    "print(f\"Shape unified matrix: {X_pca_60y.shape}\")\n",
    "\n",
    "# Save the unified matrix\n",
    "np.save(\"Mid_result_to_save/X_pca_60y.npy\", X_pca_60y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ef9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grib_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
