{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07fb362a",
   "metadata": {},
   "source": [
    "## Notebook to create the matrix with 30y EVEN with normalization and pca altready applied\n",
    "\n",
    "OUTPUT : quantities computed over the dataset 30y_EVEN  \n",
    "-  _ipca_ saved as ipca_30y.pkl for further use\n",
    "- _global_mean_ saved as global_mean_30y.npy\n",
    "- _global_std_ saved as global_std_30y.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b659e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd64558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import gc\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import importlib\n",
    "import utils.data_processing\n",
    "import utils.visualization\n",
    "importlib.reload(utils.data_processing)\n",
    "importlib.reload(utils.visualization)\n",
    "\n",
    "# Import the functions \n",
    "from utils.data_processing import prepare_data_matrix, apply_global_standardization, perform_incremental_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a65df318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring index file '../era5_1994_2008.grib.5b7b6.idx' incompatible with GRIB file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of the combined dataset:\n",
      "   • Variables: ['z', 't']\n",
      "   • Coordinates: ['number', 'time', 'step', 'isobaricInhPa', 'latitude', 'longitude', 'valid_time']\n",
      "   • Shape: FrozenMappingWarningOnValuesAccess({'time': 5479, 'isobaricInhPa': 2, 'latitude': 201, 'longitude': 321})\n"
     ]
    }
   ],
   "source": [
    "# First file\n",
    "ds1 = xr.open_dataset('../era5_1994_2008.grib', engine='cfgrib')\n",
    "ds1_even = ds1.isel(time=slice(0, None, 2))  # select only the even rows\n",
    "del ds1  \n",
    "gc.collect()\n",
    "\n",
    "# Second file\n",
    "ds2 = xr.open_dataset('../era5_2009_2023.grib', engine='cfgrib')\n",
    "ds2_even = ds2.isel(time=slice(0, None, 2))  \n",
    "del ds2  \n",
    "gc.collect()\n",
    "\n",
    "# Merge the two datasets\n",
    "ds_even = xr.concat([ds1_even, ds2_even], dim='time')\n",
    "del ds1_even, ds2_even\n",
    "gc.collect()\n",
    "\n",
    "print(\"Overview of the combined dataset:\")\n",
    "print(f\"   • Variables: {list(ds_even.data_vars.keys())}\")\n",
    "print(f\"   • Coordinates: {list(ds_even.coords.keys())}\")\n",
    "print(f\"   • Shape: {ds_even.dims}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee4a0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ds_even['z'].dtype)\n",
    "print(ds_even['t'].dtype)\n",
    "\n",
    "ds_even = ds_even.astype('float32')\n",
    "\n",
    "# Save the dataset\n",
    "ds_even.to_netcdf(\"era5_30years_even.nc\")\n",
    "\n",
    "del ds_even\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0d309a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Dataset - Variables: ['z', 't']\n",
      "Dimensions: {'time': 5479, 'pressure_z': 1, 'latitude': 201, 'longitude': 321, 'pressure_t': 1}\n",
      "Time range: 1994-01-01T00:00:00.000000000 to 2023-12-30T00:00:00.000000000\n",
      "<xarray.Dataset> Size: 3GB\n",
      "Dimensions:     (time: 5479, pressure_z: 1, latitude: 201, longitude: 321,\n",
      "                 pressure_t: 1)\n",
      "Coordinates:\n",
      "    number      int64 8B 0\n",
      "  * time        (time) datetime64[ns] 44kB 1994-01-01 1994-01-03 ... 2023-12-30\n",
      "    step        timedelta64[ns] 8B 00:00:00\n",
      "  * pressure_z  (pressure_z) float64 8B 500.0\n",
      "  * latitude    (latitude) float64 2kB 70.0 69.75 69.5 69.25 ... 20.5 20.25 20.0\n",
      "  * longitude   (longitude) float64 3kB -40.0 -39.75 -39.5 ... 39.5 39.75 40.0\n",
      "    valid_time  (time) datetime64[ns] 44kB 1994-01-01 1994-01-03 ... 2023-12-30\n",
      "  * pressure_t  (pressure_t) float64 8B 850.0\n",
      "Data variables:\n",
      "    z           (time, pressure_z, latitude, longitude) float32 1GB ...\n",
      "    t           (time, pressure_t, latitude, longitude) float32 1GB ...\n",
      "-----------------\n",
      "Processing z...\n",
      "     → z: ('time', 'pressure_z', 'latitude', 'longitude') → (5479, 64521)\n",
      "Processing t...\n",
      "     → t: ('time', 'pressure_t', 'latitude', 'longitude') → (5479, 64521)\n",
      "\n",
      "Combined matrix shape: (5479, 129042)\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataset\n",
    "ds_filtered = xr.Dataset()\n",
    "ds_even = xr.open_dataset(\"era5_30years_even.nc\")\n",
    "\n",
    "# Add the geopotential at 500 hPa\n",
    "z_500 = ds_even['z'].sel(isobaricInhPa=[500])\n",
    "\n",
    "# Add the temperature at 850 hPa\n",
    "t_850 = ds_even['t'].sel(isobaricInhPa=[850])\n",
    "\n",
    "# Rename the coordinates in order to avoid conflicts\n",
    "z_500 = z_500.rename({'isobaricInhPa': 'pressure_z'})\n",
    "t_850 = t_850.rename({'isobaricInhPa': 'pressure_t'})\n",
    "\n",
    "# Combine everything into the filtered dataset\n",
    "ds_filtered = xr.Dataset({\n",
    "    'z': z_500,\n",
    "    't': t_850\n",
    "})\n",
    "\n",
    "\n",
    "print(f\"Filtered Dataset - Variables: {list(ds_filtered.data_vars.keys())}\")\n",
    "print(f\"Dimensions: {dict(ds_filtered.dims)}\")\n",
    "print(f\"Time range: {ds_filtered.time.min().values} to {ds_filtered.time.max().values}\")\n",
    "\n",
    "print(ds_filtered)  \n",
    "\n",
    "print(\"-----------------\")\n",
    "    \n",
    "X, data_matrices = prepare_data_matrix(ds_filtered) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90281af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOBAL STANDARDIZATION\n",
      "Before standardization:\n",
      " • Global mean: 27779.39m\n",
      " • Global std: 27544.21m\n",
      "Dataset shape: (5479, 129042)\n",
      "After standardization:\n",
      " • New global mean: 0.000253\n",
      " • New global std: 1.000214\n",
      " • Sample min: -1.000\n",
      " • Sample max: 1.138\n"
     ]
    }
   ],
   "source": [
    "print(\"GLOBAL STANDARDIZATION\")\n",
    "\n",
    "# Apply global standardization\n",
    "X, global_mean, global_std = apply_global_standardization(X)\n",
    "\n",
    "print(\"Before standardization:\")\n",
    "print(f\" • Global mean: {global_mean:.2f}m\")\n",
    "print(f\" • Global std: {global_std:.2f}m\")\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "\n",
    "\n",
    "print(\"After standardization:\")\n",
    "print(f\" • New global mean: {X.mean():.6f}\")\n",
    "print(f\" • New global std: {X.std():.6f}\")\n",
    "\n",
    "# Verify some sample statistics\n",
    "print(f\" • Sample min: {X.min():.3f}\")\n",
    "print(f\" • Sample max: {X.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a46fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save mean and std\n",
    "np.save(\"global_mean_30y.npy\", global_mean)\n",
    "np.save(\"global_std_30y.npy\", global_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6acfad33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INCREMENTAL PCA ANALYSIS\n",
      "   • Target components: 20\n",
      "   • Batch size: 100\n",
      "Original shape: (5479, 129042)\n",
      "PCA shape: (5479, 20)\n",
      "\n",
      "First 10 components variance: [0.49234794 0.11733436 0.09122786 0.07440938 0.05198909 0.03327221\n",
      " 0.0219017  0.02104583 0.01374796 0.01086561]\n",
      "Total explained variance 20 components cumulative: 0.976\n"
     ]
    }
   ],
   "source": [
    "print(\"INCREMENTAL PCA ANALYSIS\")\n",
    "\n",
    "n_components = 20  # Reduced to 20 components for safety with batch processing\n",
    "batch_size = 100    # Increased batch size to accommodate more components\n",
    "\n",
    "print(f\"   • Target components: {n_components}\")\n",
    "print(f\"   • Batch size: {batch_size}\")\n",
    " \n",
    "X_pca, ipca, explained_variance_ratio, cumulative_variance = perform_incremental_pca(X, n_components=n_components, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72fc811d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ipca_30y.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the PCA model\n",
    "\n",
    "joblib.dump(ipca, \"ipca_30y.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grib_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
