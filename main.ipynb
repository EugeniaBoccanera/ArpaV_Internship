{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f09e8e9a",
   "metadata": {},
   "source": [
    "# Unsupervised Clustering Methods for Meteorological European Configurations/ Patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8275b925",
   "metadata": {},
   "source": [
    "<span style=\"color: yellow;\"> - prendo solo un sub set dei dati per la velocità (fatto in -> 1.1), poi sarà da prendre tutto il dataset (30.07.25)  </span>  \n",
    "<span style=\"color: yellow;\">- vedere se togliere i percentili in 1.1</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "595b08ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c18b7",
   "metadata": {},
   "source": [
    "## 1 Caricamento Dati e Analisi Iniziale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa42acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ds = xr.open_dataset('era5_2000_2004.grib', engine= 'cfgrib') # XArray DataSet\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "335f3534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of the dataset:\n",
      "   • Variabili: ['z', 't', 'u', 'v']\n",
      "   • Coordinate: ['number', 'time', 'step', 'isobaricInhPa', 'latitude', 'longitude', 'valid_time']\n"
     ]
    }
   ],
   "source": [
    "print(\"Overview of the dataset:\")\n",
    "print(f\"   • Variabili: {list(ds.data_vars.keys())}\")\n",
    "print(f\"   • Coordinate: {list(ds.coords.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97965aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension details:\n",
      "   • Latitude: 201 points (20.0° - 70.0°)\n",
      "   • Longitude: 321 points (-40.0° - 40.0°)\n",
      "   • Time: 1827 steps (2000-01-01 - 2004-12-31)\n",
      "   • Pressure levels: 3 levels ([np.float64(850.0), np.float64(500.0), np.float64(250.0)] hPa)\n",
      "Variables in the dataset:\n",
      "   • z: ('time', 'isobaricInhPa', 'latitude', 'longitude') - Geopotential\n",
      "     └─ Units: m**2 s**-2\n",
      "   • t: ('time', 'isobaricInhPa', 'latitude', 'longitude') - Temperature\n",
      "     └─ Units: K\n",
      "   • u: ('time', 'isobaricInhPa', 'latitude', 'longitude') - U component of wind\n",
      "     └─ Units: m s**-1\n",
      "   • v: ('time', 'isobaricInhPa', 'latitude', 'longitude') - V component of wind\n",
      "     └─ Units: m s**-1\n"
     ]
    }
   ],
   "source": [
    "# Dimenision details\n",
    "print(\"Dimension details:\")\n",
    "if 'latitude' in ds.dims:\n",
    "    print(f\"   • Latitude: {ds.dims['latitude']} points ({ds.latitude.min().values:.1f}° - {ds.latitude.max().values:.1f}°)\")\n",
    "if 'longitude' in ds.dims:\n",
    "    print(f\"   • Longitude: {ds.dims['longitude']} points ({ds.longitude.min().values:.1f}° - {ds.longitude.max().values:.1f}°)\")\n",
    "if 'time' in ds.dims:\n",
    "    print(f\"   • Time: {ds.dims['time']} steps ({pd.to_datetime(ds.time.values[0]).strftime('%Y-%m-%d')} - {pd.to_datetime(ds.time.values[-1]).strftime('%Y-%m-%d')})\")\n",
    "if 'isobaricInhPa' in ds.dims:\n",
    "    print(f\"   • Pressure levels: {ds.dims['isobaricInhPa']} levels ({list(ds.isobaricInhPa.values)} hPa)\")\n",
    "\n",
    "#Variables \n",
    "print(\"Variables in the dataset:\")\n",
    "for var in ds.data_vars:\n",
    "    var_data = ds[var]\n",
    "    print(f\"   • {var}: {var_data.dims} - {var_data.attrs.get('long_name', 'N/A')}\")\n",
    "    print(f\"     └─ Units: {var_data.attrs.get('units', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd087867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSIONALITY:\n",
      "   • Spatial points: 64521\n",
      "   • Total features per timestep: 774,252\n",
      "   • Temporal samples: 1827\n"
     ]
    }
   ],
   "source": [
    "# Total dimensionality\n",
    "total_spatial_points = 1\n",
    "for dim in ['latitude', 'longitude']:\n",
    "    if dim in ds.dims:\n",
    "        total_spatial_points *= ds.dims[dim]\n",
    "\n",
    "total_features = len(ds.data_vars) * ds.dims.get('isobaricInhPa', 1) * total_spatial_points\n",
    "print(\"DIMENSIONALITY:\")\n",
    "print(f\"   • Spatial points: {total_spatial_points}\")\n",
    "print(f\"   • Total features per timestep: {total_features:,}\")\n",
    "print(f\"   • Temporal samples: {ds.dims.get('time', 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d1633",
   "metadata": {},
   "source": [
    "Spatial points: punti griglia nello spazio. \n",
    "La regione osservata è suddivisa in una griglia regolare (0.25° x 0.25°), per ogni punto nella grigli avengono misurate le variabili\n",
    "\n",
    "Total features per timestep: numero di variabili (features) in totale in ogni istante di tempo\n",
    "\n",
    "Temporal samples: punti temporali nel dataset (365 giorni per 5 anni)\n",
    "\n",
    "Posso trasformarlo in una matrice per il clustering:  \n",
    "shape = (temporal_samples, total_features_per_timestep)\n",
    "       = (1827, 774252)  \n",
    "Ogni riga = una mappa meteorologica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5e19da",
   "metadata": {},
   "source": [
    "### 1.1 Check quality in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e70968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of the dataset for a specific time range (POI DA TOGLIERE)\n",
    "#ds = ds.sel(time=slice(\"2000-01-01\", \"2001-12-31\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a4083f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING VALUES:\n",
      "    z: 0 missing (0.00%)\n",
      "    t: 0 missing (0.00%)\n",
      "    u: 0 missing (0.00%)\n",
      "    v: 0 missing (0.00%)\n"
     ]
    }
   ],
   "source": [
    "def analyze_missing_values(dataset):\n",
    "    \"\"\"Analizzes missing values for each variable\"\"\"\n",
    "    missing_info = {}\n",
    "    \n",
    "    for var in dataset.data_vars:\n",
    "        data = dataset[var]\n",
    "        total_values = data.size\n",
    "        missing_count = np.isnan(data.values).sum()\n",
    "        missing_percent = (missing_count / total_values) * 100\n",
    "        \n",
    "        missing_info[var] = {\n",
    "            'count': missing_count,\n",
    "            'percentage': missing_percent,\n",
    "            'total': total_values\n",
    "        }\n",
    "    \n",
    "    return missing_info\n",
    "\n",
    "print(\"MISSING VALUES:\")\n",
    "missing_analysis = analyze_missing_values(ds)\n",
    "\n",
    "for var, info in missing_analysis.items():\n",
    "    print(f\"    {var}: {info['count']:,} missing ({info['percentage']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f4c385",
   "metadata": {},
   "source": [
    "There are no missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7856d465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATISTICS:\n",
      " Z:\n",
      "      • Min: 7495.887\n",
      "      • Max: 108808.312\n",
      "      • Mean: 57405.691\n",
      "      • Std: 36090.105\n",
      "      • Percentiles [25%, 50%, 75%]: [ 15097.1484375  55659.46875   100139.4375   ]\n",
      " T:\n",
      "      • Min: 198.893\n",
      "      • Max: 308.547\n",
      "      • Mean: 252.359\n",
      "      • Std: 24.976\n",
      "      • Percentiles [25%, 50%, 75%]: [226.2230072  255.91355896 272.93847656]\n",
      " U:\n",
      "      • Min: -63.877\n",
      "      • Max: 112.808\n",
      "      • Mean: 8.984\n",
      "      • Std: 13.904\n",
      "      • Percentiles [25%, 50%, 75%]: [-0.31930542  6.58242798 16.01531982]\n",
      " V:\n",
      "      • Min: -91.157\n",
      "      • Max: 89.342\n",
      "      • Mean: -0.238\n",
      "      • Std: 11.834\n",
      "      • Percentiles [25%, 50%, 75%]: [-5.98008728 -0.32400513  5.86112976]\n"
     ]
    }
   ],
   "source": [
    "print(\"STATISTICS:\")\n",
    "for var in ds.data_vars:\n",
    "    data = ds[var].values\n",
    "    valid_data = data[~np.isnan(data)]\n",
    "    \n",
    "    if len(valid_data) > 0:\n",
    "        print(f\" {var.upper()}:\")\n",
    "        print(f\"      • Min: {valid_data.min():.3f}\")\n",
    "        print(f\"      • Max: {valid_data.max():.3f}\")\n",
    "        print(f\"      • Mean: {valid_data.mean():.3f}\")\n",
    "        print(f\"      • Std: {valid_data.std():.3f}\")\n",
    "        print(f\"      • Percentiles [25%, 50%, 75%]: {np.percentile(valid_data, [25, 50, 75])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e916de27",
   "metadata": {},
   "source": [
    "## 2 Preprocessing and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4469779",
   "metadata": {},
   "source": [
    "Ho 4 variabili: z, u, v, t\n",
    "Per ogni variabile: var[time, pressure, latitude, longitude]\n",
    "\n",
    "Ogni variabile ha 64521 valori per timestamp (3 × 201 × 321 = 64521 punti spaziali)(3 lv. di pressione x 201 lat x 321 lon)  \n",
    "Con 4 variabili ⇒ ogni riga della matrice finale avrà:\n",
    "\n",
    "    4 × 64521 = 774252 colonne (features)\n",
    "\n",
    "Un timestamp è come un pacco 3D: (3, 201, 321, 4)\n",
    "Un pacco per ogni giorno dal 01/01/2000 al 31/12/2004 → 1827 pacchi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd245b9a",
   "metadata": {},
   "source": [
    "#### Struttura inziale dei dati\n",
    "\n",
    "**Scatola** = Dataset   \n",
    "dentro la scatola di sono dei blocchi di fogli\n",
    "\n",
    "**Un blocco di fogli** = un signolo giorno ( da 1 gennaio 2000 a 21 dic 2004)  -> 1827 giorni  \n",
    "il blocco di fogli è formato da 4 fogli uno per ogni variabile\n",
    "\n",
    "**Un foglio contiene i valori di una variabile** =  variabili: u, v, z, t -> 4 variabili  \n",
    "\n",
    "Ogni foglio contiene i valori di quella variabile presi in ogni singolo punto dello 'spazio' definito dalla longitudine e dalla laitudine. Quindi in ogni foglio c'è il valore di quella variabile in ognuno dei 201(lat) × 321(lon). Una specie di tabella.  -> 64521 punti spaziali\n",
    "\n",
    "**Solo che questa tabella di valori è presa per ognuno dei 3 livelli di pressione** = 850 hPa, 500 hPa, 250 hPa -> 3 lv di pressione\n",
    "\n",
    "**TOT= 774252 valori per blocco**   x 1827 giorni\n",
    "\n",
    "Per ogni variabile:  \n",
    "__per ogni livello di pressione:  \n",
    "____per ogni lat:  \n",
    "______per ogni lon:  \n",
    "________prendi il valore  \n",
    "\n",
    "Immagina il foglio come una tabella con 774252 colonne, e solo 1 riga, che rappresenta tutte le misure spaziali per quel giorno.\n",
    "Se metti insieme tutti i 1827 fogli, ottieni una matrice finale di forma (1827, 774252). Ogni riga è un giorno. Ogni colonna è una variabile a una certa posizione e pressione.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c947a7e",
   "metadata": {},
   "source": [
    "_capire come vengono modificate le dimensioni nella modifica fatta, 2 chat su chat gpt_\n",
    "\n",
    "\n",
    "L’obiettivo è trasformare tutto in una tabella 2D:\n",
    "\n",
    "           feature_1  feature_2  ...  feature_774252\n",
    "time_1 →      ...        ...             ...\n",
    "time_2 →      ...        ...             ...\n",
    "  ⋮                             \n",
    "time_1827 →   ...        ...             ...\n",
    "\n",
    "    Righe: 1827 (una per ogni timestep)\n",
    "\n",
    "    Colonne: 774,252 (una per ogni combinazione di punto spaziale × variabile)\n",
    "\n",
    "    Ogni colonna è, ad esempio:\n",
    "    \"z_850hPa_lat37.5_lon12.0\"\n",
    "    \"u_500hPa_lat42.5_lon18.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62cdc9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   • Processando z...\n",
      "   • Processando t...\n",
      "   • Processando u...\n",
      "   • Processando v...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m     clean_matrix \u001b[38;5;241m=\u001b[39m combined_matrix[mask]\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clean_matrix, data_matrices\n\u001b[0;32m---> 34\u001b[0m X, data_matrices \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Matrice dati: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (samples, features)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m, in \u001b[0;36mprepare_data_matrix\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     26\u001b[0m combined_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(all_matrices, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Rimuovi NaN\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_matrix\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39many(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m clean_matrix \u001b[38;5;241m=\u001b[39m combined_matrix[mask]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_matrix, data_matrices\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Converti dataset in array 2D per ML\n",
    "def prepare_data_matrix(dataset):\n",
    "    \"\"\"Converte xarray dataset in matrice 2D per ML\"\"\"\n",
    "    data_matrices = {}\n",
    "    \n",
    "    for var in dataset.data_vars:\n",
    "        print(f\"   • Processando {var}...\")\n",
    "        var_data = dataset[var]\n",
    "        \n",
    "        # Riorganizza dimensioni: (time, features)\n",
    "        if 'time' in var_data.dims:\n",
    "            # Stack tutte le dimensioni non temporali\n",
    "            spatial_dims = [dim for dim in var_data.dims if dim != 'time']\n",
    "            if spatial_dims:\n",
    "                stacked = var_data.stack(features=spatial_dims)\n",
    "                matrix = stacked.values  # shape: (time, features)\n",
    "            else:\n",
    "                matrix = var_data.values.reshape(-1, 1)\n",
    "        else:\n",
    "            matrix = var_data.values.flatten().reshape(1, -1)\n",
    "        \n",
    "        data_matrices[var] = matrix\n",
    "    \n",
    "    # Concatena tutte le variabili\n",
    "    all_matrices = list(data_matrices.values())\n",
    "    combined_matrix = np.concatenate(all_matrices, axis=1)\n",
    "    \n",
    "    # Rimuovi NaN\n",
    "    mask = ~np.isnan(combined_matrix).any(axis=1)\n",
    "    clean_matrix = combined_matrix[mask]\n",
    "    \n",
    "    return clean_matrix, data_matrices\n",
    "\n",
    "X, data_matrices = prepare_data_matrix(ds)\n",
    "print(f\"✅ Matrice dati: {X.shape} (samples, features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c303fb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64521"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "201*321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7295402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grib_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
