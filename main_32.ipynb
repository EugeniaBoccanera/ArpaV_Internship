{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f09e8e9a",
   "metadata": {},
   "source": [
    "# Unsupervised Clustering Methods for Meteorological European Configurations/ Patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8275b925",
   "metadata": {},
   "source": [
    "<span style=\"color: yellow;\"> - prendo solo un sub set dei dati per la velocità (fatto in -> 1.1), poi sarà da prendre tutto il dataset (30.07.25)  </span>  \n",
    "<span style=\"color: yellow;\">- vedere se togliere i percentili in 1.1</span>  \n",
    "<span style=\"color: yellow;\">- In 2.2 vedere se standard scaler va bene o se è emglio usare robust scaler, per ora dovrebbe andare bene</span>  \n",
    "<span style=\"color: yellow;\">- eventualmente nella standardizzazione posso demarcare la cella sopra che ha le funzioni per il calcolo della memoria</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "595b08ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c18b7",
   "metadata": {},
   "source": [
    "## 1 Caricamento Dati e Analisi Iniziale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa42acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ds = xr.open_dataset('era5_2000_2004.grib', engine= 'cfgrib') # XArray DataSet\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "335f3534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of the dataset:\n",
      "   • Variabili: ['z', 't', 'u', 'v']\n",
      "   • Coordinate: ['number', 'time', 'step', 'isobaricInhPa', 'latitude', 'longitude', 'valid_time']\n"
     ]
    }
   ],
   "source": [
    "print(\"Overview of the dataset:\")\n",
    "print(f\"   • Variabili: {list(ds.data_vars.keys())}\")\n",
    "print(f\"   • Coordinate: {list(ds.coords.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97965aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension details:\n",
      "   • Latitude: 201 points (20.0° - 70.0°)\n",
      "   • Longitude: 321 points (-40.0° - 40.0°)\n",
      "   • Time: 1827 steps (2000-01-01 - 2004-12-31)\n",
      "   • Pressure levels: 3 levels ([np.float64(850.0), np.float64(500.0), np.float64(250.0)] hPa)\n",
      "Variables in the dataset:\n",
      "   • z: ('time', 'isobaricInhPa', 'latitude', 'longitude') - Geopotential\n",
      "     └─ Units: m**2 s**-2\n",
      "   • t: ('time', 'isobaricInhPa', 'latitude', 'longitude') - Temperature\n",
      "     └─ Units: K\n",
      "   • u: ('time', 'isobaricInhPa', 'latitude', 'longitude') - U component of wind\n",
      "     └─ Units: m s**-1\n",
      "   • v: ('time', 'isobaricInhPa', 'latitude', 'longitude') - V component of wind\n",
      "     └─ Units: m s**-1\n"
     ]
    }
   ],
   "source": [
    "# Dimenision details\n",
    "print(\"Dimension details:\")\n",
    "if 'latitude' in ds.dims:\n",
    "    print(f\"   • Latitude: {ds.dims['latitude']} points ({ds.latitude.min().values:.1f}° - {ds.latitude.max().values:.1f}°)\")\n",
    "if 'longitude' in ds.dims:\n",
    "    print(f\"   • Longitude: {ds.dims['longitude']} points ({ds.longitude.min().values:.1f}° - {ds.longitude.max().values:.1f}°)\")\n",
    "if 'time' in ds.dims:\n",
    "    print(f\"   • Time: {ds.dims['time']} steps ({pd.to_datetime(ds.time.values[0]).strftime('%Y-%m-%d')} - {pd.to_datetime(ds.time.values[-1]).strftime('%Y-%m-%d')})\")\n",
    "if 'isobaricInhPa' in ds.dims:\n",
    "    print(f\"   • Pressure levels: {ds.dims['isobaricInhPa']} levels ({list(ds.isobaricInhPa.values)} hPa)\")\n",
    "\n",
    "#Variables \n",
    "print(\"Variables in the dataset:\")\n",
    "for var in ds.data_vars:\n",
    "    var_data = ds[var]\n",
    "    print(f\"   • {var}: {var_data.dims} - {var_data.attrs.get('long_name', 'N/A')}\")\n",
    "    print(f\"     └─ Units: {var_data.attrs.get('units', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd087867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSIONALITY:\n",
      "   • Spatial points: 64521\n",
      "   • Total features per timestep: 774,252\n",
      "   • Temporal samples: 1827\n"
     ]
    }
   ],
   "source": [
    "# Total dimensionality\n",
    "total_spatial_points = 1\n",
    "for dim in ['latitude', 'longitude']:\n",
    "    if dim in ds.dims:\n",
    "        total_spatial_points *= ds.dims[dim]\n",
    "\n",
    "total_features = len(ds.data_vars) * ds.dims.get('isobaricInhPa', 1) * total_spatial_points\n",
    "print(\"DIMENSIONALITY:\")\n",
    "print(f\"   • Spatial points: {total_spatial_points}\")\n",
    "print(f\"   • Total features per timestep: {total_features:,}\")\n",
    "print(f\"   • Temporal samples: {ds.dims.get('time', 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d1633",
   "metadata": {},
   "source": [
    "Spatial points: punti griglia nello spazio. \n",
    "La regione osservata è suddivisa in una griglia regolare (0.25° x 0.25°), per ogni punto nella grigli avengono misurate le variabili\n",
    "\n",
    "Total features per timestep: numero di variabili (features) in totale in ogni istante di tempo\n",
    "\n",
    "Temporal samples: punti temporali nel dataset (365 giorni per 5 anni)\n",
    "\n",
    "Posso trasformarlo in una matrice per il clustering:  \n",
    "shape = (temporal_samples, total_features_per_timestep)\n",
    "       = (1827, 774252)  \n",
    "Ogni riga = una mappa meteorologica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5e19da",
   "metadata": {},
   "source": [
    "### 1.1 Check quality in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a4083f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING VALUES: No missing values (previously verified)\n"
     ]
    }
   ],
   "source": [
    "# Missing values check: already verified to be 0\n",
    "print(\"MISSING VALUES: No missing values (previously verified)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f4c385",
   "metadata": {},
   "source": [
    "There are no missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7856d465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATISTICS:\n",
      "Variables found: ['z', 't', 'u', 'v']\n",
      "\n",
      "\n",
      " Z:\n",
      "      • Min: 7495.887\n",
      "      • Max: 108808.312\n",
      "      • Mean: 57405.691\n",
      "      • Std: 36090.105\n",
      "\n",
      " T:\n",
      "      • Min: 7495.887\n",
      "      • Max: 108808.312\n",
      "      • Mean: 57405.691\n",
      "      • Std: 36090.105\n",
      "\n",
      " T:\n",
      "      • Min: 198.893\n",
      "      • Max: 308.547\n",
      "      • Mean: 252.359\n",
      "      • Std: 24.976\n",
      "\n",
      " U:\n",
      "      • Min: 198.893\n",
      "      • Max: 308.547\n",
      "      • Mean: 252.359\n",
      "      • Std: 24.976\n",
      "\n",
      " U:\n",
      "      • Min: -63.877\n",
      "      • Max: 112.808\n",
      "      • Mean: 8.984\n",
      "      • Std: 13.904\n",
      "\n",
      " V:\n",
      "      • Min: -63.877\n",
      "      • Max: 112.808\n",
      "      • Mean: 8.984\n",
      "      • Std: 13.904\n",
      "\n",
      " V:\n",
      "      • Min: -91.157\n",
      "      • Max: 89.342\n",
      "      • Mean: -0.238\n",
      "      • Std: 11.834\n",
      "      • Min: -91.157\n",
      "      • Max: 89.342\n",
      "      • Mean: -0.238\n",
      "      • Std: 11.834\n"
     ]
    }
   ],
   "source": [
    "variables = list(ds.data_vars.keys())\n",
    "print(f\"STATISTICS:\\nVariables found: {variables}\\n\")\n",
    "\n",
    "for var in variables:\n",
    "    print(f\"\\n {var.upper()}:\")\n",
    "    \n",
    "    var_min = float(ds[var].min().values)\n",
    "    var_max = float(ds[var].max().values)\n",
    "    var_mean = float(ds[var].mean().values)\n",
    "    var_std = float(ds[var].std().values)\n",
    "\n",
    "    print(f\"      • Min: {var_min:.3f}\")\n",
    "    print(f\"      • Max: {var_max:.3f}\")\n",
    "    print(f\"      • Mean: {var_mean:.3f}\")\n",
    "    print(f\"      • Std: {var_std:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e916de27",
   "metadata": {},
   "source": [
    "## 2 Preprocessing and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818f3343",
   "metadata": {},
   "source": [
    "### 2.1 Preparing Data Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4469779",
   "metadata": {},
   "source": [
    "Ho 4 variabili: z, u, v, t\n",
    "Per ogni variabile: var[time, pressure, latitude, longitude]\n",
    "\n",
    "Ogni variabile ha 64521 valori per timestamp (3 × 201 × 321 = 64521 punti spaziali)(3 lv. di pressione x 201 lat x 321 lon)  \n",
    "Con 4 variabili ⇒ ogni riga della matrice finale avrà:\n",
    "\n",
    "    4 × 64521 = 774252 colonne (features)\n",
    "\n",
    "Un timestamp è come un pacco 3D: (3, 201, 321, 4)\n",
    "Un pacco per ogni giorno dal 01/01/2000 al 31/12/2004 → 1827 pacchi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd245b9a",
   "metadata": {},
   "source": [
    "#### Struttura inziale dei dati\n",
    "\n",
    "**Scatola** = Dataset   \n",
    "dentro la scatola di sono dei blocchi di fogli\n",
    "\n",
    "**Un blocco di fogli** = un signolo giorno ( da 1 gennaio 2000 a 21 dic 2004)  -> 1827 giorni  \n",
    "il blocco di fogli è formato da 4 fogli uno per ogni variabile\n",
    "\n",
    "**Un foglio contiene i valori di una variabile** =  variabili: u, v, z, t -> 4 variabili  \n",
    "\n",
    "Ogni foglio contiene i valori di quella variabile presi in ogni singolo punto dello 'spazio' definito dalla longitudine e dalla laitudine. Quindi in ogni foglio c'è il valore di quella variabile in ognuno dei 201(lat) × 321(lon). Una specie di tabella.  -> 64521 punti spaziali\n",
    "\n",
    "**Solo che questa tabella di valori è presa per ognuno dei 3 livelli di pressione** = 850 hPa, 500 hPa, 250 hPa -> 3 lv di pressione\n",
    "\n",
    "**TOT= 774252 valori per blocco**   x 1827 giorni\n",
    "\n",
    "Per ogni variabile:  \n",
    "__per ogni livello di pressione:  \n",
    "____per ogni lat:  \n",
    "______per ogni lon:  \n",
    "________prendi il valore  \n",
    "\n",
    "Immagina il foglio come una tabella con 774252 colonne, e solo 1 riga, che rappresenta tutte le misure spaziali per quel giorno.\n",
    "Se metti insieme tutti i 1827 fogli, ottieni una matrice finale di forma (1827, 774252). Ogni riga è un giorno. Ogni colonna è una variabile a una certa posizione e pressione.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c947a7e",
   "metadata": {},
   "source": [
    "#### Struttura finale dei dati\n",
    "\n",
    "L’obiettivo è trasformare tutto in una tabella 2D:\n",
    "\n",
    "           feature_1  feature_2  ...  feature_774252  \n",
    "time_1 →      ...        ...             ...  \n",
    "time_2 →      ...        ...             ...  \n",
    "  ⋮                             \n",
    "time_1827 →   ...        ...             ...  \n",
    "\n",
    "Righe: 1827 (una per ogni timestep)  \n",
    "Colonne: 774,252 (una per ogni combinazione di punto spaziale × variabile)  \n",
    "\n",
    "Ogni colonna è, ad esempio:  \n",
    "\"z_850hPa_lat37.5_lon12.0\"  \n",
    "\"u_500hPa_lat42.5_lon18.0\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62cdc9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPARING DATA MATRIX\n",
      "   • Processando z...\n",
      "   • Processando t...\n",
      "   • Processando t...\n",
      "   • Processando u...\n",
      "   • Processando u...\n",
      "   • Processando v...\n",
      "   • Processando v...\n",
      "Data Matrix: (1827, 774252) (samples, features)\n",
      "Data Matrix: (1827, 774252) (samples, features)\n"
     ]
    }
   ],
   "source": [
    "# Dataset convertito in array 2D \n",
    "def prepare_data_matrix(dataset):\n",
    "    \"\"\"Converte xarray dataset in matrice 2D con float32\"\"\"\n",
    "    data_matrices = {}\n",
    "    \n",
    "    for var in dataset.data_vars:\n",
    "        print(f\"   • Processando {var}...\")\n",
    "        var_data = dataset[var]\n",
    "        \n",
    "        if 'time' in var_data.dims:\n",
    "            spatial_dims = [dim for dim in var_data.dims if dim != 'time']\n",
    "            if spatial_dims:                                             # From: var[time=1827, pressure=3, lat=201, lon=321]\n",
    "                stacked = var_data.stack(features=spatial_dims)          # To:  var[time=1827, features=193563] \n",
    "                matrix = stacked.values.astype(np.float32)  # ← float32 dimezza memoria\n",
    "            else:\n",
    "                matrix = var_data.values.reshape(-1, 1).astype(np.float32)\n",
    "        else:\n",
    "            matrix = var_data.values.flatten().reshape(1, -1).astype(np.float32)\n",
    "        \n",
    "        data_matrices[var] = matrix\n",
    "    \n",
    "    # Concatena in float32\n",
    "    all_matrices = list(data_matrices.values())\n",
    "    combined_matrix = np.concatenate(all_matrices, axis=1)\n",
    "    \n",
    "    return combined_matrix, data_matrices\n",
    "\n",
    "print(\"PREPARING DATA MATRIX\")\n",
    "X, data_matrices = prepare_data_matrix(ds)\n",
    "print(f\"Data Matrix: {X.shape} (samples, features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39b946f",
   "metadata": {},
   "source": [
    "Prima (per la variabile z):  \n",
    "z[time=0, pressure=850, lat=37.5, lon=12.0] = 1234.5  \n",
    "z[time=0, pressure=500, lat=37.5, lon=12.0] = 5678.9  \n",
    "z[time=0, pressure=250, lat=37.5, lon=12.0] = 9876.1  \n",
    "...  \n",
    "\n",
    "Dopo lo stack:  \n",
    "z[time=0, feature_0] = 1234.5  # (850hPa, lat37.5, lon12.0)  \n",
    "z[time=0, feature_1] = 5678.9  # (500hPa, lat37.5, lon12.0)    \n",
    "z[time=0, feature_2] = 9876.1  # (250hPa, lat37.5, lon12.0)  \n",
    "...  \n",
    "\n",
    "Concatenazione finale:\n",
    "\n",
    "X[time=0] = [z_features (tti i valori di z)... | t_features... | u_features... | v_features...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4036ffae",
   "metadata": {},
   "source": [
    "### 2.2 Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314d72f9",
   "metadata": {},
   "source": [
    "Per ora uso StandardScaler poichè i valori sono ben distribuiti. Ma nel caso la PCA o il Kmeans venissero strai posso provare ad utilizzare RobustScaler che è più robusto agli outliers.\n",
    "\n",
    "Se la memoria è un problema, potresti valutare l’uso di uno scaler \"incrementale\" (sklearn.preprocessing.StandardScaler supporta partial_fit per i batch) — oppure ridurre la dimensionalità prima con PCA.  \n",
    "\n",
    "Se ho problemi di RAM, posso:\n",
    "\n",
    "    Salvare i batch su disco dopo la standardizzazione (es. con npy o HDF5).\n",
    "\n",
    "    Usare joblib o dask per gestire dati più grandi della RAM.\n",
    "\n",
    "    Ridurre le feature prima della standardizzazione (es. PCA incrementale)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f30c440",
   "metadata": {},
   "source": [
    "# PULIZIA MEMORIA E MONITORAGGIO\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Ottiene l'uso della memoria corrente\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    return memory_info.rss / 1024 / 1024 / 1024  # GB\n",
    "\n",
    "def clean_memory():\n",
    "    \"\"\"Pulisce la memoria e forza garbage collection\"\"\"\n",
    "    gc.collect()\n",
    "    print(f\"   • Memoria dopo pulizia: {get_memory_usage():.2f} GB\")\n",
    "\n",
    "print(\"PULIZIA MEMORIA PRE-STANDARDIZZAZIONE\")\n",
    "print(f\"   • Memoria iniziale: {get_memory_usage():.2f} GB\")\n",
    "print(f\"   • Dimensione matrice X: {X.nbytes / 1024 / 1024 / 1024:.2f} GB\")\n",
    "\n",
    "# Pulizia forzata\n",
    "clean_memory()\n",
    "\n",
    "# Verifica che X sia in float32\n",
    "if X.dtype != np.float32:\n",
    "    print(\"   • AVVISO: Convertendo X in float32...\")\n",
    "    X = X.astype(np.float32)\n",
    "    clean_memory()\n",
    "\n",
    "print(\"   • Memoria pronta per standardizzazione\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7295402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STANDARDIZATION - Ultra Memory-Safe (Batch size 1)\n",
      "   • Batch size: 1 (estrema sicurezza)\n",
      "   • Fase 1: Calcolo statistiche (1 campione alla volta)...\n",
      "   • partial_fit completed\n",
      "   • Fase 2: Trasformazione in-place (ZERO copie)...\n",
      "   • Standardization completed!\n",
      "   • Dataset shape: (1827, 774252)\n",
      "   • Verifica standardizzazione (mini-sample):\n",
      "     → Campione 0: mean = -0.308231\n",
      "     → Campione 100: mean = -0.152127\n",
      "     → Campione 500: mean = -0.068365\n",
      "     → Campione 1000: mean = 0.178275\n",
      "     → Campione 1500: mean = -0.324477\n",
      "   • Test prime 3 features (primi 3 campioni):\n",
      "   • Mini-sample means: [-2.7600496 -2.7559578 -2.7526739]\n",
      "   • Mini-sample stds: [0.5020489  0.49900958 0.49649367]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"STANDARDIZATION - Ultra Memory-Safe (Batch size 1)\")\n",
    "import gc\n",
    "\n",
    "# BATCH SIZE = 1 per evitare qualsiasi crash\n",
    "batch_size = 1\n",
    "print(f\"   • Batch size: {batch_size} \")\n",
    "#print(f\"   • Memoria pre-fit: {get_memory_usage():.2f} GB\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Step 1: Calcolo statistiche un campione alla volta\n",
    "print(\"   • Fase 1: Calcolo statistiche (1 campione alla volta)...\")\n",
    "for i in range(0, X.shape[0], batch_size):\n",
    "    # Evita ANY copia - usa slice diretto\n",
    "    scaler.partial_fit(X[i:i+1])\n",
    "    \n",
    "    # Log ogni 100 campioni per evitare spam\n",
    "    if i % 100 == 0:\n",
    "        progress = (i / X.shape[0]) * 100\n",
    "        #print(f\"     → Progresso fit: {progress:.1f}% ({i}/{X.shape[0]}) - Memoria: {get_memory_usage():.2f} GB\")\n",
    "        gc.collect()\n",
    "\n",
    "print(\"   • partial_fit completed\")\n",
    "#clean_memory()\n",
    "\n",
    "# Step 2: Trasformazione diretta in-place (ZERO copie temporanee)\n",
    "print(\"   • Fase 2: Trasformazione in-place (ZERO copie)...\")\n",
    "for i in range(0, X.shape[0]):\n",
    "    # Trasformazione diretta su singola riga SENZA variabili temporanee\n",
    "    X[i:i+1] = scaler.transform(X[i:i+1])\n",
    "    \n",
    "    # Log ogni 100 campioni\n",
    "    if i % 100 == 0:\n",
    "        progress = (i / X.shape[0]) * 100\n",
    "        #print(f\"     → Progresso transform: {progress:.1f}% ({i}/{X.shape[0]}) - Memoria: {get_memory_usage():.2f} GB\")\n",
    "\n",
    "        # Garbage collection ogni 200 campioni\n",
    "        if i % 200 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "print(f\"   • Standardization completed!\")\n",
    "#clean_memory()\n",
    "\n",
    "print(f\"   • Dataset shape: {X.shape}\")\n",
    "#print(f\"   • Memoria finale: {get_memory_usage():.2f} GB\")\n",
    "\n",
    "# Verifica su sample ultra-piccolo\n",
    "print(\"   • Verifica standardizzazione (mini-sample):\")\n",
    "# Solo 5 campioni casuali per evitare sovraccarico\n",
    "test_indices = [0, 100, 500, 1000, 1500]\n",
    "for idx in test_indices:\n",
    "    if idx < X.shape[0]:\n",
    "        sample_mean = X[idx].mean()\n",
    "        print(f\"     → Campione {idx}: mean = {sample_mean:.6f}\")\n",
    "\n",
    "# Test prime 3 features su primi 3 campioni\n",
    "print(\"   • Test prime 3 features (primi 3 campioni):\")\n",
    "mini_sample = X[:3, :3]\n",
    "print(f\"   • Mini-sample means: {mini_sample.mean(axis=0)}\")\n",
    "print(f\"   • Mini-sample stds: {mini_sample.std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3993b94",
   "metadata": {},
   "source": [
    "Stato della Standardizzazione:  \n",
    "Globalmente corretta: La maggior parte dei campioni ha media ≈ 0  \n",
    "Alcune variazioni: Normali per dati meteorologici reali  \n",
    "Deviazione standard ≈ 0.5: Ragionevole per dati standardizzati  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grib_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
